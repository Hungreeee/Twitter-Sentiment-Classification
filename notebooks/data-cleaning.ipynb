{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning notebook\n",
    "\n",
    "This notebook aims to provide scripts to clean the data before encoding them in the next modelling stage. The cleaning process contains in the removal/replacement of elements that can negatively affect the transformation steps later on. For this notebook, the detailed outline for the cleaning process can be given by:\n",
    " \n",
    "- Expanding contractions: Expand contractions (don't, won't, etc.) into their full form (do not, will not, etc.). This helps with the later negation handling and the stop words cleaning stage later on. \n",
    "- Removing noise texts (emails, hashtags, username mentions, links, words with repeating last letter): Remove noise text from our data. These do not contain or contain little meaningful information for the model to learn given the context of sentiment analysis, and should be removed for better focus on the main features of the text.\n",
    "- Removing stopwords: Remove words that are too frequently appeared, but contain little information or sentiment and can cause loss of focus onto the main feature of the text. \n",
    "- Removing non-alphabets (digits, special characters, punctuations mark...): Remove any character that is a letter in the English alphabet. There is no need to include every single characters into our vocabulary and make it more complex. While it may affect the information conveyed by the text, this is still quite insignificant as it is very likely to contain sentiment and should be ignored for the sake of simplicity and efficiency.\n",
    "- Negation handling: Add negation indicators ('NEG' tag) as the prefix of the word immediately following the negation word (such as 'not', 'no', etc.). The basic idea of negation handling is to reflect the negation polarity of a sentence, which can be accidentally ignored by the cleaning process if we don't pay attention to it. This is often an neglected task but is actually an important aspect to consider, especially when we are working with bag-of-words model where positions and relations of words do not matter. \n",
    "- Additional task (Lowercasing and redundant spaces removing): While lowercasing letters can give us a smaller vocabulary to work with, removing redudant spaces (which can potentially confuse the tokenizer) will improve consistency of setences. \n",
    "\n",
    "Given that the data is from Twitter, where there is no standard for typing, the data can be very messy and should definitely be handled with a thorough cleaning process. At the same time, it is also important to remember that because we are working with such a large data set, the optimization aspects should also be considered so that the computational time-cost is suitable with our machine strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hung\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Hung\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Hung\n",
      "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import contractions\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by loading in the Twitter sentiment data set and label the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   label   1599999 non-null  int64 \n",
      " 1   id      1599999 non-null  int64 \n",
      " 2   date    1599999 non-null  object\n",
      " 3   query   1599999 non-null  object\n",
      " 4   user    1599999 non-null  object\n",
      " 5   text    1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../data/twitter-data.csv', encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "\n",
    "# Define column names for the data\n",
    "df.columns = [\"label\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe no null values from any columns, no null-hanlding is needed for the moment. \n",
    "\n",
    "Given our modelling goal, we will only use the two columns `label`, `text` and ignore the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  is upset that he can't update his Facebook by ...\n",
       "1      0  @Kenichan I dived many times for the ball. Man...\n",
       "2      0    my whole body feels itchy and like its on fire \n",
       "3      0  @nationwideclass no, it's not behaving at all....\n",
       "4      0                      @Kwesidei not the whole crew "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the needed columns\n",
    "df = df[['label', 'text']]\n",
    "\n",
    "# View the first 5 rows of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to the main cleaning process, let us now implement functions to perform the process we discussed at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expand contractions**\n",
    "\n",
    "This can simply be done using the `contractions` library. It can handle various cases which can be easily confused if one try to use a rule-based method using ReGex instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str):\n",
    "  return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not know where he will go. Maybe he is going to Lindy's place?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"I dont know where he'll go. Maybe he's going to Lindy's place?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing noise texts (stopwords, emails, hashtags, username mentions, links)**\n",
    "\n",
    "Here, to remove/replace the noise texts, we simply define their representations in ReGex to identify and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise_texts(text: str):\n",
    "  # Patterns of the expressions we want to remove\n",
    "  email_re = r'\\S+@\\S+'\n",
    "  mention_hashtag_re = r'(@|#)\\S+'\n",
    "  link_re = r'((www\\.\\S+)|(https?://\\S+))|(\\S+\\.com)'\n",
    "  repeating_suff_re   = r'(.)\\1\\1+'\n",
    "\n",
    "  # Remove patterns in our textual data\n",
    "  text = re.sub(repeating_suff_re, r'\\1', text)\n",
    "  text = re.sub(email_re, '', text)\n",
    "  text = re.sub(mention_hashtag_re, '', text)\n",
    "  text = re.sub(link_re, '', text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In  a setence can have an email  it also mentions  and has words with repeating last letter to express my feelings. Lastly '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_noise_texts(\"In twitter.com a setence can have an email email_123@abcd.nlp.com, it also mentions @someone_on_Twitter and has words with repeating last letter to express my feelingssssssssssss. Lastly #DontForgetTheHashTag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords removal**\n",
    "\n",
    "For removing the stopwords, we will first remove every word that is included in the nltk stop words list but the negation indicators 'not' and 'no', which will be used for negation handling later. Only after that, 'not' and 'no' can be finally discarded by using this same function once again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nltk stopwords list. \n",
    "# This should be loaded outside so that it does not get re-loaded every time we call the function.\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(text: str, neg_keep: bool = False):\n",
    "  # Remove 'not' 'no' from the list if neg is True. Otherwise keep the list as it is.nh\n",
    "  stop_word_list = [word for word in stop_words if word not in ['not', 'no', 'nor']] if neg_keep else stop_words\n",
    "\n",
    "  # Filter out words that are not in the list\n",
    "  text = ' '.join([word for word in text.split() if word not in stop_word_list])\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like food'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('I do not like the food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I not like food'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('I do not like the food', neg_keep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negation handling**\n",
    "\n",
    "The basic idea of negation handling is to reflect the negation polarity of a sentence, which can be accidentally ignored by the cleaning process if we don't pay attention to it. In the removal of stopwords, we also remove negation indicators such as 'not' and 'no', thus removing completely the negation meaning of the sentence. We can clearly see this on the example of the previous function `remove_stopwords`.\n",
    "\n",
    "If we dig further into negation handling, it can be a quite complicated task and would deviates from the main aim of this project. Therefore, let us only perform the very surface level of negation handling using a simple rule-based method: *Adding a prefix 'NEG' to every word followed immediately by a negation indicator*. By doing this, we are essentially creating new words, which can ensure that words in negation contexts are not treated the same as their opposite cases by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_handle(text: str):\n",
    "  text = re.sub(r'\\b(?:not|no|never|cannot)\\b\\s+[a-z]+', \n",
    "      lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG\\2', match.group(0)), \n",
    "      text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not NEGlike the food. I like the movie.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negation_handle('I do not like the food. I like the movie.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing non-alphabets (digits, special characters, punctuations mark...)**\n",
    "\n",
    "We can simply do this using ReGex by removing any character that is not an alphabet a-z, or A-Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha(text: str):\n",
    "  # Pattern of the expression we want to remove\n",
    "  non_alpha_digit_re = r'[^a-zA-Z]'\n",
    "\n",
    "  # Remove patterns in our textual data\n",
    "  text = re.sub(non_alpha_digit_re, r' ', text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pair of shoes for        pounds      No sir good bye '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_non_alpha('Pair of shoes for 100000 pounds????? No sir good bye!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing redundant spaces**\n",
    "\n",
    "We can simply do this using ReGex by replacing more than two consecutive spaces with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text: str):\n",
    "  multi_spaces_re = r'\\s\\s+'\n",
    "  text = re.sub(multi_spaces_re, ' ', text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This has too many spaces.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_spaces(\"This         has too   many spaces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing short words**\n",
    "\n",
    "Removing short words help to reduce the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(text: str):\n",
    "  text = ' '.join(word for word in text.split() if len(word) > 1)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming or lemmatizing (*optional*)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatizing can help with reducing the complexity of our vocabulary by truncating them to simpler format or converting words to their root form. However, in this project, they will not be used. Given that we are dealing with a large corpus with many poorly formatted, unconventional words, and even typos, overstemming can occur and cause errors in model's predictions. While lemmatizing can still be useful to convert words to original forms, it has been removed due to its inability to significantly improve my machine learning model performance, while consuming a large amount of computational time-cost.\n",
    "\n",
    "Nonetheless, a lemmatizer is still implemented here and it is up to readers to use it or not. It simply performs pos-tagging on text, then lemmatize words using the that pos tag as a parameter. Pos-tagging is essential in lemmatization task, because it helps the model to understand properly what type of word it is dealing with (verb, adjective, noun, etc.). Ignoring this part is a quite common error in many projects, but this is understandable due to its high computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text: str):\n",
    "  split_text = pos_tag(text.split())\n",
    "  result = []\n",
    "  \n",
    "  for word, tag in split_text:\n",
    "    postag = tag[0].lower()\n",
    "    postag = postag if postag in ['a', 'n', 'v', 'r'] else None\n",
    "    if not postag:\n",
    "      result.append(word)\n",
    "    else:\n",
    "      result.append(lemmatizer.lemmatize(word, pos=postag)) \n",
    "\n",
    "  return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I eat hamburger yesterday.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize('I ate hamburgers yesterday.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final cleaning function**\n",
    "\n",
    "Now, we can put the created functions (with additional cleaning as mentioned) back together in the following order:\n",
    "\n",
    "Lowercasing words $\\rightarrow$ Expand contractions $\\rightarrow$ Removing noise texts $\\rightarrow$ Removing stopwords but negation stopwords $\\rightarrow$ Negation handling $\\rightarrow$ Removing non-alphabets $\\rightarrow$ Removing short words $\\rightarrow$ Removing redundant spaces $\\rightarrow$ Removing all stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text: str, handle_neg=False, remove_all_stopword=False, is_lemmatized=False):\n",
    "  text = text.lower()\n",
    "  text = expand_contractions(text)\n",
    "  text = remove_noise_texts(text)\n",
    "  if handle_neg:\n",
    "    text = remove_stopwords(text, neg_keep=True)\n",
    "    text = negation_handle(text)\n",
    "  text = remove_non_alpha(text)\n",
    "  text = remove_short_words(text)\n",
    "  text = remove_spaces(text)\n",
    "  if is_lemmatized:\n",
    "    text = lemmatize(text)\n",
    "  if remove_all_stopword:\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "  return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just got an email from think it is troll he told me in the mail that he really do not like my dog but he did like cat'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning(\"Just got an email from prankster@troll.com, I think it's @Jordan's troll. He told me in the mail that he really don't like my dog, but he did like a cat. #IHateJordan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now proceed to apply this to the `text` column. Note that the cleaning function might remove all of our text, leaving only empty strings behind. Thus, we should also remove them from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning function to text column\n",
    "df['neg_handled_text'] = df['text'].apply(lambda row: data_cleaning(row, handle_neg=True, remove_all_stopword=True))\n",
    "\n",
    "# Apply cleaning function to text column\n",
    "df['normal_cleaned_text'] = df['text'].apply(data_cleaning)\n",
    "\n",
    "# Remove rows with no characters\n",
    "df_neg = df[df['neg_handled_text'] != ''][['neg_handled_text', 'label']]\n",
    "df_normal = df[df['normal_cleaned_text'] != ''][['normal_cleaned_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg_handled_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upset cannot NEGupdate facebook texting might ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dived many times ball managed save rest go bounds</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEGbehaving mad cannot NEGsee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEGwhole crew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    neg_handled_text  label\n",
       "0  upset cannot NEGupdate facebook texting might ...      0\n",
       "1  dived many times ball managed save rest go bounds      0\n",
       "2                   whole body feels itchy like fire      0\n",
       "3                      NEGbehaving mad cannot NEGsee      0\n",
       "4                                      NEGwhole crew      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 5 rows of the cleaned data\n",
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_cleaned_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is upset that he cannot update his facebook by...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dived many times for the ball managed to save ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no it is not behaving at all am mad why am her...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 normal_cleaned_text  label\n",
       "0  is upset that he cannot update his facebook by...      0\n",
       "1  dived many times for the ball managed to save ...      0\n",
       "2     my whole body feels itchy and like its on fire      0\n",
       "3  no it is not behaving at all am mad why am her...      0\n",
       "4                                 not the whole crew      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 5 rows of the cleaned data\n",
    "df_normal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can finally transform our data into a feature vector `X`, and a target vector `y`. They will be saved into a folder as `numpy` arrays for later usage in the modelling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neg = df_neg['neg_handled_text'].values\n",
    "y_neg = df_neg['label'].values\n",
    "X_normal = df_normal['normal_cleaned_text'].values\n",
    "y_normal = df_normal['label'].values\n",
    "\n",
    "np.save('../data/preprocessed_data/feature_vectors_normal.npy', X_normal)\n",
    "np.save('../data/preprocessed_data/target_vectors_normal.npy', y_normal)\n",
    "np.save('../data/preprocessed_data/feature_vectors_neg.npy', X_neg)\n",
    "np.save('../data/preprocessed_data/target_vectors_neg.npy', y_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
